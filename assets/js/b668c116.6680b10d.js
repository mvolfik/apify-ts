"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[3978],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>c});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(n),c=i,k=m["".concat(s,".").concat(c)]||m[c]||u[c]||l;return n?a.createElement(k,r(r({ref:t},d),{},{components:n})):a.createElement(k,r({ref:t},d))}));function c(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var l=n.length,r=new Array(l);r[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:i,r[1]=o;for(var p=2;p<l;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},8690:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>p,toc:()=>u});var a=n(7462),i=n(3366),l=(n(7294),n(3905)),r=["components"],o={id:"upgrading-to-v3",title:"Upgrading to v3"},s=void 0,p={unversionedId:"upgrading/upgrading-to-v3",id:"upgrading/upgrading-to-v3",title:"Upgrading to v3",description:"This page summarizes most of the breaking changes between Crawlee (v3) and Apify SDK (v2). Crawlee is the spiritual successor to Apify SDK, so we decided to keep the versioning and release Crawlee as v3.",source:"@site/../docs/upgrading/upgrading_v3.md",sourceDirName:"upgrading",slug:"/upgrading/upgrading-to-v3",permalink:"/apify-ts/docs/upgrading/upgrading-to-v3",draft:!1,tags:[],version:"current",lastUpdatedBy:"Martin Ad\xe1mek",lastUpdatedAt:1654606074,formattedLastUpdatedAt:"6/7/2022",frontMatter:{id:"upgrading-to-v3",title:"Upgrading to v3"},sidebar:"docs",previous:{title:"Upgrading to v2",permalink:"/apify-ts/docs/upgrading/upgrading-to-v2"}},d={},u=[{value:"Crawlee vs Apify SDK",id:"crawlee-vs-apify-sdk",level:2},{value:"Installing Crawlee",id:"installing-crawlee",level:3},{value:"Full TypeScript support",id:"full-typescript-support",level:2},{value:"Docker build",id:"docker-build",level:3},{value:"Browser fingerprints",id:"browser-fingerprints",level:2},{value:"Memory storage",id:"memory-storage",level:2},{value:"Purging of the default storage",id:"purging-of-the-default-storage",level:2},{value:"Renamed crawler options and interfaces",id:"renamed-crawler-options-and-interfaces",level:2},{value:"Context aware helpers",id:"context-aware-helpers",level:2},{value:"Enqueuing links",id:"enqueuing-links",level:3},{value:"Implicit <code>RequestQueue</code> instance",id:"implicit-requestqueue-instance",level:2},{value:"<code>crawler.addRequests()</code>",id:"crawleraddrequests",level:2},{value:"Less verbose error logging",id:"less-verbose-error-logging",level:2},{value:"Removal of <code>requestAsBrowser</code>",id:"removal-of-requestasbrowser",level:2},{value:"Handling requests outside of browser",id:"handling-requests-outside-of-browser",level:2},{value:"Logging",id:"logging",level:2},{value:"Actor SDK",id:"actor-sdk",level:2},{value:"Smaller/internal breaking changes",id:"smallerinternal-breaking-changes",level:2}],m={toc:u};function c(e){var t=e.components,n=(0,i.Z)(e,r);return(0,l.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("p",null,"This page summarizes most of the breaking changes between Crawlee (v3) and Apify SDK (v2). Crawlee is the spiritual successor to Apify SDK, so we decided to keep the versioning and release Crawlee as v3."),(0,l.kt)("h2",{id:"crawlee-vs-apify-sdk"},"Crawlee vs Apify SDK"),(0,l.kt)("p",null,"Up until version 3 of ",(0,l.kt)("inlineCode",{parentName:"p"},"apify"),", the package contained both scraping related tools and Apify platform related helper methods. With v3 we are splitting the whole project into two main parts:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Crawlee, the new web-scraping library, available as ",(0,l.kt)("inlineCode",{parentName:"li"},"crawlee")," package on NPM"),(0,l.kt)("li",{parentName:"ul"},"Actor SDK, helpers for the Apify platform, available as ",(0,l.kt)("inlineCode",{parentName:"li"},"apify")," package on NPM")),(0,l.kt)("p",null,"Moreover, the Crawlee library is published as several packages under ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee")," namespace:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/core"),": the base for all the crawler implementations, also contains things like ",(0,l.kt)("inlineCode",{parentName:"li"},"Request"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"RequestQueue"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"RequestList")," or ",(0,l.kt)("inlineCode",{parentName:"li"},"Dataset")," classes"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/basic"),": exports ",(0,l.kt)("inlineCode",{parentName:"li"},"BasicCrawler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/cheerio"),": exports ",(0,l.kt)("inlineCode",{parentName:"li"},"CheerioCrawler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/browser"),": exports ",(0,l.kt)("inlineCode",{parentName:"li"},"BrowserCrawler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/playwright"),": exports ",(0,l.kt)("inlineCode",{parentName:"li"},"PlaywrightCrawler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/puppeteer"),": exports ",(0,l.kt)("inlineCode",{parentName:"li"},"PuppeteerCrawler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/memory-storage"),": ",(0,l.kt)("inlineCode",{parentName:"li"},"@apify/storage-local")," alternative"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/browser-pool"),": previously ",(0,l.kt)("inlineCode",{parentName:"li"},"browser-pool")," package"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/utils"),": utility methods"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"@crawlee/types"),": holds TS interfaces mainly about the ",(0,l.kt)("inlineCode",{parentName:"li"},"StorageClient"))),(0,l.kt)("h3",{id:"installing-crawlee"},"Installing Crawlee"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"As Crawlee is not yet released as ",(0,l.kt)("inlineCode",{parentName:"p"},"latest"),", we need to install from the ",(0,l.kt)("inlineCode",{parentName:"p"},"next")," distribution tag!")),(0,l.kt)("p",null,"Most of the Crawlee packages are extending and reexporting each other, so it's enough to install just the one you plan on using, e.g. ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/playwright")," if you plan on using ",(0,l.kt)("inlineCode",{parentName:"p"},"playwright")," - it already contains everything from the ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/browser")," package, which includes everything from ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/basic"),", which includes everything from ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/core"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"npm install @crawlee/cheerio@next\n")),(0,l.kt)("p",null,"When using ",(0,l.kt)("inlineCode",{parentName:"p"},"playwright")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"puppeteer"),", we still need to install those dependencies explicitly - this allows the users to be in control of which version will be used."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"npm install @crawlee/playwright@next playwright\n")),(0,l.kt)("p",null,"Alternatively we can also use the ",(0,l.kt)("inlineCode",{parentName:"p"},"crawlee")," meta-package which contains (re-exports) most of the ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/*")," packages, and therefore contains all the crawler classes."),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Sometimes you might want to use some utility methods from ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/utils"),", so you might want to install that as well. This package contains some utilities that were previously available under ",(0,l.kt)("inlineCode",{parentName:"p"},"Apify.utils"),". Browser related utilities can be also found in the crawler packages (e.g. ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/playwright"),").")),(0,l.kt)("h2",{id:"full-typescript-support"},"Full TypeScript support"),(0,l.kt)("p",null,"Both Crawlee and Actor SDK are full TypeScript rewrite, so they include up-to-date types in the package. For your TypeScript crawlers we recommend using our predefined TypeScript configuration from ",(0,l.kt)("inlineCode",{parentName:"p"},"@apify/tsconfig")," package. Don't forget to set the ",(0,l.kt)("inlineCode",{parentName:"p"},"module")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"target")," to ",(0,l.kt)("inlineCode",{parentName:"p"},"ES2022")," or above to be able to use top level await."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="tsconfig.json"',title:'"tsconfig.json"'},'{\n    "extends": "@apify/tsconfig",\n    "compilerOptions": {\n        "module": "ES2022",\n        "target": "ES2022",\n        "outDir": "dist",\n        "lib": ["DOM"]\n    },\n    "include": [\n        "./src/**/*"\n    ]\n}\n')),(0,l.kt)("h3",{id:"docker-build"},"Docker build"),(0,l.kt)("p",null,"For ",(0,l.kt)("inlineCode",{parentName:"p"},"Dockerfile")," we recommend using multi-stage build so you don't install the dev dependencies like TypeScript in your final image:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-dockerfile",metastring:'title="Dockerfile"',title:'"Dockerfile"'},'# using multistage build, as we need dev deps to build the TS source code\nFROM apify/actor-node:16 AS builder\n\n# copy all files, install all dependencies (including dev deps) and build the project\nCOPY . ./\nRUN npm install --include=dev \\\n    && npm run build\n\n# create final image\nFROM apify/actor-node:16\n# copy only necessary files\nCOPY --from=builder /usr/src/app/package*.json ./\nCOPY --from=builder /usr/src/app/README.md ./\nCOPY --from=builder /usr/src/app/dist ./dist\nCOPY --from=builder /usr/src/app/apify.json ./apify.json\nCOPY --from=builder /usr/src/app/INPUT_SCHEMA.json ./INPUT_SCHEMA.json\n\n# install only prod deps\nRUN npm --quiet set progress=false \\\n    && npm install --only=prod --no-optional \\\n    && echo "Installed NPM packages:" \\\n    && (npm list --only=prod --no-optional --all || true) \\\n    && echo "Node.js version:" \\\n    && node --version \\\n    && echo "NPM version:" \\\n    && npm --version\n\n# run compiled code\nCMD npm run start:prod\n')),(0,l.kt)("h2",{id:"browser-fingerprints"},"Browser fingerprints"),(0,l.kt)("p",null,"Previously we had a magical ",(0,l.kt)("inlineCode",{parentName:"p"},"stealth")," option in the puppeteer crawler that enabled several tricks aiming to mimic the real users as much as possible. While this worked to a certain degree, we decided to replace it with generated browser fingerprints."),(0,l.kt)("p",null,"In case we don't want to have dynamic fingerprints, we can disable this behaviour via ",(0,l.kt)("inlineCode",{parentName:"p"},"useFingerprints")," in ",(0,l.kt)("inlineCode",{parentName:"p"},"browserPoolOptions"),":"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"const crawler = new PlaywrightCrawler({\n   browserPoolOptions: {\n       useFingerprints: false,\n   },\n});\n")),(0,l.kt)("h2",{id:"memory-storage"},"Memory storage"),(0,l.kt)("p",null,"When we store some data or intermediate state (like the one ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," holds), we now use ",(0,l.kt)("inlineCode",{parentName:"p"},"@crawlee/memory-storage")," by default. It is an alternative to the ",(0,l.kt)("inlineCode",{parentName:"p"},"@apify/storage-local"),", that stores the state inside memory (as opposed to SQLite database used by ",(0,l.kt)("inlineCode",{parentName:"p"},"@apify/storage-local"),"). While the state is stored in memory, it also dumps it to the file system so we can observe it, as well as respects the existing data stored in KeyValueStore (e.g. the ",(0,l.kt)("inlineCode",{parentName:"p"},"INPUT.json")," file)."),(0,l.kt)("p",null,"When we want to run the crawler on Apify platform, we need to use ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.main"),", which will automatically switch the storage client to ",(0,l.kt)("inlineCode",{parentName:"p"},"ApifyClient")," when on the Apify platform."),(0,l.kt)("p",null,"We can still use the ",(0,l.kt)("inlineCode",{parentName:"p"},"@apify/storage-local"),", to do it, first install it pass it to the ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.main")," options:"),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("inlineCode",{parentName:"p"},"@apify/storage-local")," v2.1.0+ is required for crawlee")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"import { Actor } from 'apify';\nimport { ApifyStorageLocal } from '@apify/storage-local';\n\nconst storage = new ApifyStorageLocal(/* options like `enableWalMode` belong here */);\nawait Actor.init({ storage });\n")),(0,l.kt)("h2",{id:"purging-of-the-default-storage"},"Purging of the default storage"),(0,l.kt)("p",null,"Previously the state was preserved between local runs, and we had to use ",(0,l.kt)("inlineCode",{parentName:"p"},"--purge")," argument of the ",(0,l.kt)("inlineCode",{parentName:"p"},"apify-cli"),". With Crawlee, this is now the default behaviour, we purge the storage automatically on ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init/main")," call. We can opt out of it via ",(0,l.kt)("inlineCode",{parentName:"p"},"purge: false")," in the ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init")," options."),(0,l.kt)("h2",{id:"renamed-crawler-options-and-interfaces"},"Renamed crawler options and interfaces"),(0,l.kt)("p",null,"Some options were renamed to better reflect what they do. We still support all the old parameter names too, but not at the TS level."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"handleRequestFunction")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"requestHandler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"handlePageFunction")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"requestHandler")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"handleRequestTimeoutSecs")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"requestHandlerTimeoutSecs")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"handlePageTimeoutSecs")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"requestHandlerTimeoutSecs")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"handleFailedRequestFunction")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"failedRequestHandler"))),(0,l.kt)("p",null,"We also renamed the crawling context interfaces, so they follow the same convention and are more meaningful:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"CheerioHandlePageInputs")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"CheerioCrawlingContext")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"PlaywrightHandlePageFunction")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"PlaywrightCrawlingContext")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"PuppeteerHandlePageFunction")," -> ",(0,l.kt)("inlineCode",{parentName:"li"},"PuppeteerCrawlingContext"))),(0,l.kt)("h2",{id:"context-aware-helpers"},"Context aware helpers"),(0,l.kt)("p",null,"Some utilities previously available under ",(0,l.kt)("inlineCode",{parentName:"p"},"Apify.utils")," namespace are now moved to the crawling context and are ",(0,l.kt)("em",{parentName:"p"},"context aware"),". This means they have some parameters automatically filled in from the context, like the current ",(0,l.kt)("inlineCode",{parentName:"p"},"Request")," instance or current ",(0,l.kt)("inlineCode",{parentName:"p"},"Page")," object, or the ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," bound to the crawler."),(0,l.kt)("h3",{id:"enqueuing-links"},"Enqueuing links"),(0,l.kt)("p",null,"One common helper that received more attention is the ",(0,l.kt)("inlineCode",{parentName:"p"},"enqueueLinks"),". As mentioned above, it is context aware - we no longer need pass in the ",(0,l.kt)("inlineCode",{parentName:"p"},"requestQueue")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"page")," arguments (or the cheerio handle ",(0,l.kt)("inlineCode",{parentName:"p"},"$"),"). In addition to that, it now offers 3 enqueuing strategies:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"EnqueueStrategy.All")," (",(0,l.kt)("inlineCode",{parentName:"li"},"'all'"),"): Matches any URLs found"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"EnqueueStrategy.SameSubdomain")," (",(0,l.kt)("inlineCode",{parentName:"li"},"'same-subdomain'"),") Matches any URLs that have the same subdomain as the base URL (default)"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"EnqueueStrategy.SameHostname")," (",(0,l.kt)("inlineCode",{parentName:"li"},"'same-hostname'"),") Matches any URLs that have the same hostname. For example, ",(0,l.kt)("inlineCode",{parentName:"li"},"https://wow.an.example.com")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"https://example.com")," will both be matched for a base url of ",(0,l.kt)("inlineCode",{parentName:"li"},"https://example.com"),".")),(0,l.kt)("p",null,"This means we can even call ",(0,l.kt)("inlineCode",{parentName:"p"},"enqueueLinks()")," without any parameters. By default, it will go through all the links found on current page and filter only those targeting the same subdomain."),(0,l.kt)("p",null,"Moreover, we can specify patterns the URL should match via globs:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"const crawler = new PlaywrightCrawler({\n    async requestHandler({ enqueueLinks }) {\n        await enqueueLinks({\n            globs: ['https://apify.com/*/*'],\n            // we can also use `regexps` and `pseudoUrls` keys here\n        });\n    },\n});\n")),(0,l.kt)("h2",{id:"implicit-requestqueue-instance"},"Implicit ",(0,l.kt)("inlineCode",{parentName:"h2"},"RequestQueue")," instance"),(0,l.kt)("p",null,"All crawlers now have the ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," instance automatically available via ",(0,l.kt)("inlineCode",{parentName:"p"},"crawler.getRequestQueue()")," method. It will create the instance for you if it does not exist yet. This mean we no longer need to create the ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," instance manually, and we can just use ",(0,l.kt)("inlineCode",{parentName:"p"},"crawler.addRequests()")," method described underneath."),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"We can still create the ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," explicitly, the ",(0,l.kt)("inlineCode",{parentName:"p"},"crawler.getRequestQueue()")," method will respect that and return the instance provided via crawler options.")),(0,l.kt)("h2",{id:"crawleraddrequests"},(0,l.kt)("inlineCode",{parentName:"h2"},"crawler.addRequests()")),(0,l.kt)("p",null,"We can now add multiple requests in batches. The newly added ",(0,l.kt)("inlineCode",{parentName:"p"},"addRequests")," method will handle everything for us. It enqueues the first 1000 requests and resolves, while continuing with the rest in the background, again in a smaller 1000 items batches, so we don't fall into any API rate limits. This means the crawling will start almost immediately (within few seconds at most), something previously possible only with a combination of ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestQueue")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"RequestList"),"."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"// will resolve right after the initial batch of 1000 requests is added\nconst result = await crawler.addRequests([/* many requests, can be even millions */]);\n\n// if we want to wait for all the requests to be added, we can await the `waitForAllRequestsToBeAdded` promise\nawait result.waitForAllRequestsToBeAdded;\n")),(0,l.kt)("h2",{id:"less-verbose-error-logging"},"Less verbose error logging"),(0,l.kt)("p",null,"Previously an error throw from inside request handler resulted in full error object being logger. With crawlee, we log only the error message as a warning as long as we know the request will be retried. If you want to enable verbose logging as in v2, use ",(0,l.kt)("inlineCode",{parentName:"p"},"CRAWLEE_VERBOSE_LOG")," env var."),(0,l.kt)("h2",{id:"removal-of-requestasbrowser"},"Removal of ",(0,l.kt)("inlineCode",{parentName:"h2"},"requestAsBrowser")),(0,l.kt)("p",null,"In v1 we replaced the underlying implementation of ",(0,l.kt)("inlineCode",{parentName:"p"},"requestAsBrowser")," to be just a proxy over calling ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/apify/got-scraping",target:"_blank",rel:"noopener"},(0,l.kt)("inlineCode",{parentName:"a"},"got-scraping"))," - our custom extension to ",(0,l.kt)("inlineCode",{parentName:"p"},"got")," that tries to mimic the real browsers as much as possible. With v3, we are removing the ",(0,l.kt)("inlineCode",{parentName:"p"},"requestAsBrowser"),", encouraging the use of ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/apify/got-scraping",target:"_blank",rel:"noopener"},(0,l.kt)("inlineCode",{parentName:"a"},"got-scraping"))," directly."),(0,l.kt)("p",null,"For easier migration, we also added ",(0,l.kt)("inlineCode",{parentName:"p"},"context.sendRequest()")," helper that allows processing our ",(0,l.kt)("inlineCode",{parentName:"p"},"Request")," object instances through ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/apify/got-scraping",target:"_blank",rel:"noopener"},(0,l.kt)("inlineCode",{parentName:"a"},"got-scraping")),":"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"const crawler = new BasicCrawler({\n    async requestHandler({ sendRequest }) {\n        // by default it will use the `Request` object from current context\n        const res = await sendRequest(/* new Request(...) */);\n    },\n});\n")),(0,l.kt)("h2",{id:"handling-requests-outside-of-browser"},"Handling requests outside of browser"),(0,l.kt)("p",null,"One small feature worth mentioning is the ability to handle requests with browser crawlers outside the browser. To do that, we can use a combination of ",(0,l.kt)("inlineCode",{parentName:"p"},"Request.skipNavigation")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"context.sendRequest()"),"."),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},(0,l.kt)("strong",{parentName:"p"},"TODO")," add example and link it from here")),(0,l.kt)("h2",{id:"logging"},"Logging"),(0,l.kt)("p",null,"Crawlee exports the default ",(0,l.kt)("inlineCode",{parentName:"p"},"log")," instance directly as a named export. We also have a scoped ",(0,l.kt)("inlineCode",{parentName:"p"},"log")," instance provided in the crawling context - this one will log messages prefixed with the crawler name and should be preferred for logging inside the request handler."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"const crawler = new CheerioCrawler({\n    async requestHandler({ log, request }) {\n        log.info(`Opened ${request.loadedUrl}`);\n    },\n});\n")),(0,l.kt)("h2",{id:"actor-sdk"},"Actor SDK"),(0,l.kt)("p",null,"The Apify platform helpers can be now found in the Actor SDK (",(0,l.kt)("inlineCode",{parentName:"p"},"apify")," NPM package). It exports the ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor")," class that offers following static helpers:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"ApifyClient")," shortcuts: ",(0,l.kt)("inlineCode",{parentName:"li"},"addWebhook()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"call()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"callTask()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"metamorph()")),(0,l.kt)("li",{parentName:"ul"},"helpers for running on Apify platform: ",(0,l.kt)("inlineCode",{parentName:"li"},"init()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"exit()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"fail()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"main()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"isAtHome()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"createProxyConfiguration()")),(0,l.kt)("li",{parentName:"ul"},"storage support: ",(0,l.kt)("inlineCode",{parentName:"li"},"getInput()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"getValue()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"openDataset()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"openKeyValueStore()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"openRequestList()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"openRequestQueue()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"pushData()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"setValue()")),(0,l.kt)("li",{parentName:"ul"},"events support: ",(0,l.kt)("inlineCode",{parentName:"li"},"on()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"off()")),(0,l.kt)("li",{parentName:"ul"},"other utilities: ",(0,l.kt)("inlineCode",{parentName:"li"},"getEnv()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"newClient()"),", ",(0,l.kt)("inlineCode",{parentName:"li"},"reboot()"))),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"Actor.main")," is now just a syntax sugar around calling ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init()")," at the beginning and ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.exit()")," at the end (plus wrapping the user function in try/catch block). All those methods are async and should be awaited - with node 16 we can use the top level await for that. In other words, following is equivalent:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"import { Actor } from 'apify';\n\nawait Actor.init();\n// your code\nawait Actor.exit('Crawling finished!');\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-ts"},"import { Actor } from 'apify';\n\nawait Actor.main(async () => {\n    // your code\n}, { statusMessage: 'Crawling finished!' });\n")),(0,l.kt)("p",null,(0,l.kt)("inlineCode",{parentName:"p"},"Actor.init()")," will conditionally set the storage implementation of Crawlee to the ",(0,l.kt)("inlineCode",{parentName:"p"},"ApifyClient")," when running on the Apify platform, or keep the default (memory storage) implementation otherwise. It will also subscribe to the websocket events (or mimic them locally). ",(0,l.kt)("inlineCode",{parentName:"p"},"Actor.exit()")," will handle the tear down and calls ",(0,l.kt)("inlineCode",{parentName:"p"},"process.exit()")," to ensure our process won't hang indefinitely for some reason."),(0,l.kt)("h2",{id:"smallerinternal-breaking-changes"},"Smaller/internal breaking changes"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Apify.call()")," is now just a shortcut for running ",(0,l.kt)("inlineCode",{parentName:"li"},"ApifyClient.actor(actorId).call(input, options)"),", while also taking the token inside env vars into account"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Apify.callTask()")," is now just a shortcut for running ",(0,l.kt)("inlineCode",{parentName:"li"},"ApifyClient.task(taskId).call(input, options)"),", while also taking the token inside env vars into account"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Apify.metamorph()")," is now just a shortcut for running ",(0,l.kt)("inlineCode",{parentName:"li"},"ApifyClient.task(taskId).metamorph(input, options)"),", while also taking the ACTOR_RUN_ID inside env vars into account"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Apify.waitForRunToFinish()")," has been removed, use ",(0,l.kt)("inlineCode",{parentName:"li"},"ApifyClient.waitForFinish()")," instead"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Actor.main/init")," purges the storage by default"),(0,l.kt)("li",{parentName:"ul"},"remove ",(0,l.kt)("inlineCode",{parentName:"li"},"purgeLocalStorage")," helper, move purging to the storage class directly",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"StorageClient")," interface now has optional ",(0,l.kt)("inlineCode",{parentName:"li"},"purge")," method"),(0,l.kt)("li",{parentName:"ul"},"purging happens automatically via ",(0,l.kt)("inlineCode",{parentName:"li"},"Actor.init()")," (you can opt out via ",(0,l.kt)("inlineCode",{parentName:"li"},"purge: false")," in the options of ",(0,l.kt)("inlineCode",{parentName:"li"},"init/main")," methods)"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"QueueOperationInfo.request")," is no longer available"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Request.handledAt")," is now string date in ISO format"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"Request.inProgress")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"Request.reclaimed")," are now ",(0,l.kt)("inlineCode",{parentName:"li"},"Set"),"s instead of POJOs"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"injectUnderscore")," from puppeteer utils has been removed"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"APIFY_MEMORY_MBYTES")," is no longer taken into account, use ",(0,l.kt)("inlineCode",{parentName:"li"},"AVAILABLE_MEMORY_RATIO")," instead"),(0,l.kt)("li",{parentName:"ul"},"some ",(0,l.kt)("inlineCode",{parentName:"li"},"AutoscaledPool")," options are no longer available:",(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"cpuSnapshotIntervalSecs")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"memorySnapshotIntervalSecs")," has been replaced with top level ",(0,l.kt)("inlineCode",{parentName:"li"},"systemInfoIntervalMillis")," configuration"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"maxUsedCpuRatio")," has been moved to the top level configuration"))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"ProxyConfiguration.newUrlFunction")," can be async. ",(0,l.kt)("inlineCode",{parentName:"li"},".newUrl()")," and ",(0,l.kt)("inlineCode",{parentName:"li"},".newProxyInfo()")," now return promises."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"prepareRequestFunction")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"postResponseFunction")," options are removed, use navigation hooks instead"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"gotoFunction")," and ",(0,l.kt)("inlineCode",{parentName:"li"},"gotoTimeoutSecs")," are removed"),(0,l.kt)("li",{parentName:"ul"},"removed compatibility fix for old/broken request queues with null ",(0,l.kt)("inlineCode",{parentName:"li"},"Request")," props")))}c.isMDXComponent=!0}}]);